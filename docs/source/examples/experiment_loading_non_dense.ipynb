{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d26bec45",
   "metadata": {},
   "source": [
    "# Experiment Notebook\n",
    "Load .onnx and Verify Embedding without ML-Commons API to see if the problem is with ML-Commons API or the .onnx file itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7fcb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../../..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d30d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"TracerWarning: torch.tensor\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"using SSL with verify_certs=False is insecure.\")\n",
    "\n",
    "import opensearch_py_ml as oml\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearch_py_ml.ml_models import SentenceTransformerModel\n",
    "# import mlcommon to later register the model to OpenSearch Cluster\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5942be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_URL = 'https://localhost:9200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837bd375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_os_client(cluster_url = CLUSTER_URL,\n",
    "                  username='admin',\n",
    "                  password='admin'):\n",
    "    '''\n",
    "    Get OpenSearch client\n",
    "    :param cluster_url: cluster URL like https://ml-te-netwo-1s12ba42br23v-ff1736fa7db98ff2.elb.us-west-2.amazonaws.com:443\n",
    "    :return: OpenSearch client\n",
    "    '''\n",
    "    client = OpenSearch(\n",
    "        hosts=[cluster_url],\n",
    "        http_auth=(username, password),\n",
    "        verify_certs=False\n",
    "    )\n",
    "    return client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9219ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/opensearchpy/connection/http_urllib3.py:199: UserWarning: Connecting to https://localhost:9200 using SSL with verify_certs=False is insecure.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "client = get_os_client()\n",
    "\n",
    "# Connect to ml_common client with OpenSearch client\n",
    "ml_client = MLCommonClient(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b935d5",
   "metadata": {},
   "source": [
    "## Trace the Model in Onnx Using save_as_onnx\n",
    "See `opensearch_py_ml/ml_models/sentencetransformermodel.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9f10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    "folder_path='sentence-transformers-onxx/multi-qa-mpnet-base-cos-v1'\n",
    "model_name = str(model_id.split(\"/\")[-1] + \".onnx\")\n",
    "model_path = os.path.join(folder_path, \"onnx\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f270c53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)e891a/.gitattributes: 100%|███████████████████████████████████████████████████████████████████████| 737/737 [00:00<00:00, 121kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████████████████████████████████████████████████████████████████| 190/190 [00:00<00:00, 32.1kB/s]\n",
      "Downloading (…)92a80e891a/README.md: 100%|██████████████████████████████████████████████████████████████████| 9.19k/9.19k [00:00<00:00, 4.56MB/s]\n",
      "Downloading (…)a80e891a/config.json: 100%|███████████████████████████████████████████████████████████████████████| 571/571 [00:00<00:00, 316kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 66.8kB/s]\n",
      "Downloading (…)91a/data_config.json: 100%|██████████████████████████████████████████████████████████████████| 25.5k/25.5k [00:00<00:00, 12.5MB/s]\n",
      "Downloading pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████| 438M/438M [00:01<00:00, 287MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|████████████████████████████████████████████████████████████████████| 53.0/53.0 [00:00<00:00, 9.33kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████| 239/239 [00:00<00:00, 137kB/s]\n",
      "Downloading (…)e891a/tokenizer.json: 100%|████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 16.3MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████| 363/363 [00:00<00:00, 207kB/s]\n",
      "Downloading (…)891a/train_script.py: 100%|██████████████████████████████████████████████████████████████████| 13.9k/13.9k [00:00<00:00, 7.04MB/s]\n",
      "Downloading (…)92a80e891a/vocab.txt: 100%|████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 1.59MB/s]\n",
      "Downloading (…)80e891a/modules.json: 100%|███████████████████████████████████████████████████████████████████████| 349/349 [00:00<00:00, 198kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX opset version set to: 15\n",
      "Loading pipeline (model: sentence-transformers/multi-qa-mpnet-base-cos-v1, tokenizer: sentence-transformers/multi-qa-mpnet-base-cos-v1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|███████████████████████████████████████████████████████████████████████| 571/571 [00:00<00:00, 332kB/s]\n",
      "Downloading pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████| 438M/438M [00:01<00:00, 302MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████| 363/363 [00:00<00:00, 203kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 3.16MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 6.32MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████| 239/239 [00:00<00:00, 133kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder sentence-transformers-onxx/multi-qa-mpnet-base-cos-v1/onnx\n",
      "Using framework PyTorch: 1.13.1+cu117\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_1 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "position_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask']\n",
      "model file is saved to  sentence-transformers-onxx/multi-qa-mpnet-base-cos-v1/onnx/multi-qa-mpnet-base-cos-v1.onnx\n",
      "zip file is saved to  sentence-transformers-onxx/multi-qa-mpnet-base-cos-v1/multi-qa-mpnet-base-cos-v1.zip \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case I: Initiate SentenceTransformerModel and Call save_as_onnx\n",
    "\n",
    "pre_trained_model = SentenceTransformerModel(model_id=model_id, folder_path=folder_path, overwrite=True)\n",
    "model_path_onnx = pre_trained_model.save_as_onnx(model_id=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae1109d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case II: Repeat what save_as_onnx function does\n",
    "\n",
    "# from transformers.convert_graph_to_onnx import convert\n",
    "# from pathlib import Path\n",
    "\n",
    "# model = SentenceTransformer(model_id)\n",
    "# folder_path='sentence-transformers-onxx/distiluse-base-multilingual-cased-v1'\n",
    "\n",
    "# model_name = str(model_id.split(\"/\")[-1] + \".onnx\")\n",
    "\n",
    "# model_path = os.path.join(folder_path, \"onnx\", model_name)\n",
    "        \n",
    "# convert(\n",
    "#     framework=\"pt\",\n",
    "#     model=model_id,\n",
    "#     output=Path(model_path),\n",
    "#     opset=15,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c474d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case III: Already run demo_tracing_model_torch_script_onnx_dense notebook \n",
    "\n",
    "# Skip to next step since we already have .onnx at model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f8e004",
   "metadata": {},
   "source": [
    "## Load Onnx Model to Check Our .onnx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b72e10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(model_path)\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d3b848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a human readable representation of the graph\n",
    "# print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509fe53a",
   "metadata": {},
   "source": [
    "## Verify Embedidngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aded6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93709208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "input_sentences = [\"first sentence\", \"second sentence\", \"very very long random sentence for testing\"]\n",
    "autotokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "auto_features = autotokenizer(\n",
    "            input_sentences, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b997b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetTokenizerFast(name_or_path='sentence-transformers/multi-qa-mpnet-base-cos-v1', vocab_size=30527, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autotokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "782b63e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0, 2038, 6255,    2,    1,    1,    1,    1,    1],\n",
       "        [   0, 2121, 6255,    2,    1,    1,    1,    1,    1],\n",
       "        [   0, 2204, 2204, 2150, 6725, 6255, 2009, 5608,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d70b4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {k: v.cpu().detach().numpy() for k, v in auto_features.items()}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48211560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[   0, 2038, 6255,    2,    1,    1,    1,    1,    1],\n",
       "        [   0, 2121, 6255,    2,    1,    1,    1,    1,    1],\n",
       "        [   0, 2204, 2204, 2150, 6725, 6255, 2009, 5608,    2]]),\n",
       " 'attention_mask': array([[1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc7807",
   "metadata": {},
   "source": [
    "# Wrong Output Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92631c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ort_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ffbec21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ort_outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "046fdd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ce0ae87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ort_outs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "495a38f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outs[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2e0598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "original_pre_trained_model = SentenceTransformer(model_id) # From Huggingface\n",
    "original_embedding_data = list(\n",
    "    original_pre_trained_model.encode(input_sentences, convert_to_numpy=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "355e6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_data_onnx = [\n",
    "            ort_outs[1][i]\n",
    "            for i in range(len(input_sentences))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8de2d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=0.001, atol=1e-05\n\nMismatched elements: 768 / 768 (100%)\nMax absolute difference: 0.26189557\nMax relative difference: 238.45244\n x: array([ 0.084509, -0.025992, -0.011085,  0.018524, -0.006226,  0.0201  ,\n       -0.028214, -0.038654, -0.023896,  0.022964,  0.077998, -0.024981,\n        0.014886,  0.003473,  0.013484,  0.00244 ,  0.016318,  0.024735,...\n y: array([ 9.101760e-02,  4.704750e-02, -1.231821e-01, -8.055858e-03,\n       -5.737207e-02,  9.479519e-02,  1.622807e-02, -2.144495e-02,\n        6.364862e-02, -6.209429e-02,  1.010141e-02,  1.183928e-01,...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_sentences)):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_embedding_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_data_onnx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-03\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-05\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py:862\u001b[0m, in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict)\u001b[0m\n\u001b[1;32m    858\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(remarks)\n\u001b[1;32m    859\u001b[0m         msg \u001b[38;5;241m=\u001b[39m build_err_msg([ox, oy], err_msg,\n\u001b[1;32m    860\u001b[0m                             verbose\u001b[38;5;241m=\u001b[39mverbose, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m    861\u001b[0m                             names\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m), precision\u001b[38;5;241m=\u001b[39mprecision)\n\u001b[0;32m--> 862\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=0.001, atol=1e-05\n\nMismatched elements: 768 / 768 (100%)\nMax absolute difference: 0.26189557\nMax relative difference: 238.45244\n x: array([ 0.084509, -0.025992, -0.011085,  0.018524, -0.006226,  0.0201  ,\n       -0.028214, -0.038654, -0.023896,  0.022964,  0.077998, -0.024981,\n        0.014886,  0.003473,  0.013484,  0.00244 ,  0.016318,  0.024735,...\n y: array([ 9.101760e-02,  4.704750e-02, -1.231821e-01, -8.055858e-03,\n       -5.737207e-02,  9.479519e-02,  1.622807e-02, -2.144495e-02,\n        6.364862e-02, -6.209429e-02,  1.010141e-02,  1.183928e-01,..."
     ]
    }
   ],
   "source": [
    "for i in range(len(input_sentences)):\n",
    "    print(i)\n",
    "    print(np.testing.assert_allclose(original_embedding_data[i], embedding_data_onnx[i], rtol=1e-03, atol=1e-05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f863f7",
   "metadata": {},
   "source": [
    "## More Info: convert function\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/convert_graph_to_onnx.py#L387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55a5a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert(\n",
    "#     framework: str,\n",
    "#     model: str,\n",
    "#     output: Path,\n",
    "#     opset: int,\n",
    "#     tokenizer: Optional[str] = None,\n",
    "#     use_external_format: bool = False,\n",
    "#     pipeline_name: str = \"feature-extraction\",\n",
    "#     **model_kwargs,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Convert the pipeline object to the ONNX Intermediate Representation (IR) format\n",
    "\n",
    "#     Args:\n",
    "#         framework: The framework the pipeline is backed by (\"pt\" or \"tf\")\n",
    "#         model: The name of the model to load for the pipeline\n",
    "#         output: The path where the ONNX graph will be stored\n",
    "#         opset: The actual version of the ONNX operator set to use\n",
    "#         tokenizer: The name of the model to load for the pipeline, default to the model's name if not provided\n",
    "#         use_external_format:\n",
    "#             Split the model definition from its parameters to allow model bigger than 2GB (PyTorch only)\n",
    "#         pipeline_name: The kind of pipeline to instantiate (ner, question-answering, etc.)\n",
    "#         model_kwargs: Keyword arguments to be forwarded to the model constructor\n",
    "\n",
    "#     Returns:\n",
    "\n",
    "#     \"\"\"\n",
    "#     warnings.warn(\n",
    "#         \"The `transformers.convert_graph_to_onnx` package is deprecated and will be removed in version 5 of\"\n",
    "#         \" Transformers\",\n",
    "#         FutureWarning,\n",
    "#     )\n",
    "#     print(f\"ONNX opset version set to: {opset}\")\n",
    "\n",
    "#     # Load the pipeline\n",
    "#     nlp = load_graph_from_args(pipeline_name, framework, model, tokenizer, **model_kwargs)\n",
    "\n",
    "#     if not output.parent.exists():\n",
    "#         print(f\"Creating folder {output.parent}\")\n",
    "#         makedirs(output.parent.as_posix())\n",
    "#     elif len(listdir(output.parent.as_posix())) > 0:\n",
    "#         raise Exception(f\"Folder {output.parent.as_posix()} is not empty, aborting conversion\")\n",
    "\n",
    "#     # Export the graph\n",
    "#     if framework == \"pt\":\n",
    "#         convert_pytorch(nlp, opset, output, use_external_format)\n",
    "#     else:\n",
    "#         convert_tensorflow(nlp, opset, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aadab6",
   "metadata": {},
   "source": [
    "## More Info: Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e3d9a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"output_0\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 768\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"output_1\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 768\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model.graph.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9bd7d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"input_ids\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       ", name: \"attention_mask\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 7\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_param: \"batch\"\n",
       "      }\n",
       "      dim {\n",
       "        dim_param: \"sequence\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model.graph.input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a203d",
   "metadata": {},
   "source": [
    "## More Info: Modules (convert function calls load_graph_from_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fd367cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipeline (model: sentence-transformers/multi-qa-mpnet-base-cos-v1, tokenizer: sentence-transformers/multi-qa-mpnet-base-cos-v1)\n"
     ]
    }
   ],
   "source": [
    "from transformers.convert_graph_to_onnx import load_graph_from_args\n",
    "nlp = load_graph_from_args(\"feature-extraction\", \"pt\", model_id, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f00e8757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.model.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d67d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/serialization\n",
    "# https://github.com/oborchers/sentence-transformers/blob/master/examples/onnx_inference/onnx_inference.ipynb\n",
    "# https://github.com/UKPLab/sentence-transformers/pull/668"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f0d31",
   "metadata": {},
   "source": [
    "## Compare with Embedding from ML-Commons API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "855e6c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-commons_model_config.json file is saved at :  sentence-transformers-onxx/multi-qa-mpnet-base-cos-v1/ml-commons_model_config.json\n",
      "Total number of chunks 44\n",
      "Sha1 value of the model file:  6bc4472985de4ddbbc219e77c9d02d84c79aa00a3beb0000b7e73eecafe44518\n",
      "Model meta data was created successfully. Model Id:  XJIi54kBqzTvNQeAz1gs\n",
      "uploading chunk 1 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 2 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 3 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 4 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 5 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 6 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 7 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 8 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 9 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 10 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 11 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 12 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 13 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 14 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 15 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 16 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 17 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 18 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 19 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 20 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 21 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 22 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 23 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 24 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 25 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 26 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 27 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 28 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 29 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 30 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 31 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 32 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 33 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 34 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 35 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 36 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 37 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 38 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 39 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 40 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 41 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 42 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 43 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 44 of 44\n",
      "Model id: {'status': 'Uploaded'}\n",
      "Model registered successfully\n",
      "Task ID: XZIj54kBqzTvNQeAC1hj\n",
      "Model deployed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'XJIi54kBqzTvNQeAz1gs'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_model.make_model_config_json(model_format='ONNX')\n",
    "model_config_path_onnx = 'sentence-transformers-onxx/multi-qa-mpnet-base-cos-v1/ml-commons_model_config.json'\n",
    "ml_client.register_model(model_path_onnx, model_config_path_onnx, isVerbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea1a3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_commons_embedding_output_onnx = ml_client.generate_embedding(\"XJIi54kBqzTvNQeAz1gs\", input_sentences)\n",
    "ml_commons_embedding_data_onnx = [\n",
    "            ml_commons_embedding_output_onnx[\"inference_results\"][i][\"output\"][0][\"data\"]\n",
    "            for i in range(len(input_sentences))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7b62667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=0.001, atol=1e-05\n\nMismatched elements: 768 / 768 (100%)\nMax absolute difference: 0.26189553\nMax relative difference: 238.45251557\n x: array([ 0.084509, -0.025992, -0.011085,  0.018524, -0.006226,  0.020099,\n       -0.028215, -0.038654, -0.023896,  0.022964,  0.077998, -0.024981,\n        0.014886,  0.003473,  0.013484,  0.00244 ,  0.016318,  0.024735,...\n y: array([ 9.101760e-02,  4.704750e-02, -1.231821e-01, -8.055858e-03,\n       -5.737207e-02,  9.479519e-02,  1.622807e-02, -2.144495e-02,\n        6.364862e-02, -6.209429e-02,  1.010141e-02,  1.183928e-01,...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_sentences)):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_commons_embedding_data_onnx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_data_onnx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-03\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-05\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py:862\u001b[0m, in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict)\u001b[0m\n\u001b[1;32m    858\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(remarks)\n\u001b[1;32m    859\u001b[0m         msg \u001b[38;5;241m=\u001b[39m build_err_msg([ox, oy], err_msg,\n\u001b[1;32m    860\u001b[0m                             verbose\u001b[38;5;241m=\u001b[39mverbose, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m    861\u001b[0m                             names\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m), precision\u001b[38;5;241m=\u001b[39mprecision)\n\u001b[0;32m--> 862\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=0.001, atol=1e-05\n\nMismatched elements: 768 / 768 (100%)\nMax absolute difference: 0.26189553\nMax relative difference: 238.45251557\n x: array([ 0.084509, -0.025992, -0.011085,  0.018524, -0.006226,  0.020099,\n       -0.028215, -0.038654, -0.023896,  0.022964,  0.077998, -0.024981,\n        0.014886,  0.003473,  0.013484,  0.00244 ,  0.016318,  0.024735,...\n y: array([ 9.101760e-02,  4.704750e-02, -1.231821e-01, -8.055858e-03,\n       -5.737207e-02,  9.479519e-02,  1.622807e-02, -2.144495e-02,\n        6.364862e-02, -6.209429e-02,  1.010141e-02,  1.183928e-01,..."
     ]
    }
   ],
   "source": [
    "for i in range(len(input_sentences)):\n",
    "    print(i)\n",
    "    print(np.testing.assert_allclose(ml_commons_embedding_data_onnx[i], embedding_data_onnx[i], rtol=1e-03, atol=1e-05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41df2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84acf1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opensearch-py opensearch-py-ml\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../../..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be470a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"TracerWarning: torch.tensor\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"using SSL with verify_certs=False is insecure.\")\n",
    "\n",
    "import opensearch_py_ml as oml\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearch_py_ml.ml_models import SentenceTransformerModel\n",
    "# import mlcommon to later register the model to OpenSearch Cluster\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf84d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_URL = 'http://localhost:9200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596910c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_os_client(cluster_url = CLUSTER_URL,\n",
    "                  username='admin',\n",
    "                  password='admin'):\n",
    "    '''\n",
    "    Get OpenSearch client\n",
    "    :param cluster_url: cluster URL like https://ml-te-netwo-1s12ba42br23v-ff1736fa7db98ff2.elb.us-west-2.amazonaws.com:443\n",
    "    :return: OpenSearch client\n",
    "    '''\n",
    "    client = OpenSearch(\n",
    "        hosts=[cluster_url],\n",
    "        http_auth=(username, password),\n",
    "        verify_certs=False\n",
    "    )\n",
    "    return client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724f3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_os_client()\n",
    "\n",
    "# Connect to ml_common client with OpenSearch client\n",
    "ml_client = MLCommonClient(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae95155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_null_truncation_field(\n",
    "    save_json_folder_path: str,\n",
    "    max_length: int,\n",
    ") -> None:\n",
    "    tokenizer_file_path = os.path.join(save_json_folder_path, \"tokenizer.json\")\n",
    "    with open(tokenizer_file_path) as user_file:\n",
    "        parsed_json = json.load(user_file)\n",
    "    if \"truncation\" not in parsed_json or parsed_json[\"truncation\"] is None:\n",
    "        parsed_json[\"truncation\"] = {\n",
    "                \"direction\": \"Right\",\n",
    "                \"max_length\": max_length,\n",
    "                \"strategy\": \"LongestFirst\",\n",
    "                \"stride\": 0,\n",
    "        }\n",
    "        with open(tokenizer_file_path, \"w\") as file:\n",
    "            json.dump(parsed_json, file, indent=2)\n",
    "                \n",
    "def trace_cross_encoder(\n",
    "    model_id,\n",
    "    input_examples,\n",
    "    folder_path\n",
    "):\n",
    "    # cross_encoder_model = CrossEncoder(model_id, device='cpu') # Could not get name of python class object\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model_name = str(model_id.split(\"/\")[-1] + \".pt\")\n",
    "    model_path = os.path.join(folder_path, model_name)\n",
    "    save_json_folder_path = folder_path\n",
    "    model_output_path = folder_path\n",
    "    zip_file_name = str(model_id.split(\"/\")[-1] + \".zip\")\n",
    "    zip_file_path = os.path.join(model_output_path, zip_file_name)\n",
    "\n",
    "    if tokenizer.model_max_length == 1000000000000000019884624838656:\n",
    "        tokenizer.model_max_length = model.get_max_seq_length()\n",
    "        print(\n",
    "            f\"The model_max_length is not properly defined in tokenizer_config.json. Setting it to be {tokenizer.model_max_length}\"\n",
    "        )\n",
    "\n",
    "    model.save_pretrained(save_json_folder_path)\n",
    "    tokenizer.save_pretrained(save_json_folder_path)\n",
    "    fill_null_truncation_field(\n",
    "            save_json_folder_path, tokenizer.model_max_length\n",
    "    )\n",
    "\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    cpu_model = model.to(device)\n",
    "    features = tokenizer(\n",
    "        input_examples, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    print(features)\n",
    "\n",
    "    compiled_model = torch.jit.trace(\n",
    "        cpu_model,\n",
    "        (\n",
    "            (\n",
    "                features[\"input_ids\"],\n",
    "                features[\"token_type_ids\"],\n",
    "                features[\"attention_mask\"],\n",
    "            )\n",
    "        ),\n",
    "        strict=False,\n",
    "    )\n",
    "    torch.jit.save(compiled_model, model_path)\n",
    "    print(\"model file is saved to \", model_path)\n",
    "\n",
    "    # zip model file along with tokenizer.json as output\n",
    "    with ZipFile(str(zip_file_path), \"w\") as zipObj:\n",
    "        zipObj.write(\n",
    "            model_path,\n",
    "            arcname=str(model_name),\n",
    "        )\n",
    "        zipObj.write(\n",
    "            os.path.join(save_json_folder_path, \"tokenizer.json\"),\n",
    "            arcname=\"tokenizer.json\",\n",
    "        )\n",
    "    print(\"zip file is saved to \", zip_file_path, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90c2a4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 23032,   102, 20423,  2487,   102],\n",
      "        [  101, 23032,   102, 20423,  2475,   102],\n",
      "        [  101, 23032,   102, 20423,  2509,   102]]), 'token_type_ids': tensor([[0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1]])}\n",
      "model file is saved to  cross-encoder/ms-marco-MiniLM-L-12-v2/ms-marco-MiniLM-L-12-v2.pt\n",
      "zip file is saved to  cross-encoder/ms-marco-MiniLM-L-12-v2/ms-marco-MiniLM-L-12-v2.zip \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "\n",
    "model_id = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    "input_examples = [('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')]\n",
    "trace_cross_encoder(model_id, input_examples, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "687076fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.jit.load(\"cross-encoder/ms-marco-MiniLM-L-12-v2/ms-marco-MiniLM-L-12-v2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ecd2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "134f2761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2129,  2116,  2111,  2444,  1999,  4068,  1029,   102,  2129,\n",
       "          2116,  2111,  2444,  1999,  4068,  1029,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  4068,  2038,  1037,  2313,  1997,  1017,  1010, 19611,  1010,\n",
       "          6021,  2487,  5068,  4864,  1999,  2019,  2181,  1997,  6486,  2487,\n",
       "          1012,  6445,  2675,  7338,  1012,   102,  2047,  2259,  2103,  2003,\n",
       "          3297,  2005,  1996,  4956,  2688,  1997,  2396,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [('How many people live in Berlin?', 'How many people live in Berlin?'), ('Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.')]\n",
    "test_features = tokenizer(test_sentences,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f6db0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[ 4.7880],\n",
       "         [-9.6987]], grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_prediction = loaded_model(**test_features)\n",
    "pt_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc8a42cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.7880187, -9.698673 ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder(model_id)\n",
    "original_embedding = model.predict(test_sentences)\n",
    "original_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a78dce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.7880],\n",
      "        [-9.6987]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "features = tokenizer(test_sentences,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dfd195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_cross_encoder_onnx(\n",
    "    model_id,\n",
    "    folder_path\n",
    "):\n",
    "    # cross_encoder_model = CrossEncoder(model_id, device='cpu') # Could not get name of python class object\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model_name = str(model_id.split(\"/\")[-1] + \".onnx\")\n",
    "    model_path = os.path.join(folder_path, \"onnx\", model_name)\n",
    "    save_json_folder_path = folder_path\n",
    "    model_output_path = folder_path\n",
    "    zip_file_name = str(model_id.split(\"/\")[-1] + \".zip\")\n",
    "    zip_file_path = os.path.join(model_output_path, zip_file_name)\n",
    "\n",
    "    if tokenizer.model_max_length == 1000000000000000019884624838656:\n",
    "        tokenizer.model_max_length = model.get_max_seq_length()\n",
    "        print(\n",
    "            f\"The model_max_length is not properly defined in tokenizer_config.json. Setting it to be {tokenizer.model_max_length}\"\n",
    "        )\n",
    "\n",
    "    model.save_pretrained(save_json_folder_path)\n",
    "    tokenizer.save_pretrained(save_json_folder_path)\n",
    "    fill_null_truncation_field(\n",
    "            save_json_folder_path, tokenizer.model_max_length\n",
    "    )\n",
    "\n",
    "\n",
    "    convert(\n",
    "            framework=\"pt\",\n",
    "            model=model_id,\n",
    "            output=Path(model_path),\n",
    "            opset=15,\n",
    "    )\n",
    "    print(\"model file is saved to \", model_path)\n",
    "\n",
    "    # zip model file along with tokenizer.json as output\n",
    "    with ZipFile(str(zip_file_path), \"w\") as zipObj:\n",
    "        zipObj.write(\n",
    "            model_path,\n",
    "            arcname=str(model_name),\n",
    "        )\n",
    "        zipObj.write(\n",
    "            os.path.join(save_json_folder_path, \"tokenizer.json\"),\n",
    "            arcname=\"tokenizer.json\",\n",
    "        )\n",
    "    print(\"zip file is saved to \", zip_file_path, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02acabc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX opset version set to: 15\n",
      "Loading pipeline (model: cross-encoder/ms-marco-MiniLM-L-12-v2, tokenizer: cross-encoder/ms-marco-MiniLM-L-12-v2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cross-encoder/ms-marco-MiniLM-L-12-v2 were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating folder onnx/onnx\n",
      "Using framework PyTorch: 2.0.1\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input token_type_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_1 with shape: {0: 'batch'}\n",
      "Ensuring inputs are in correct order\n",
      "position_ids is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask', 'token_type_ids']\n",
      "================ Diagnostic Run torch.onnx.export version 2.0.1 ================\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "model file is saved to  onnx/onnx/ms-marco-MiniLM-L-12-v2.onnx\n",
      "zip file is saved to  onnx/ms-marco-MiniLM-L-12-v2.zip \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers.convert_graph_to_onnx import convert\n",
    "from pathlib import Path\n",
    "trace_cross_encoder_onnx(model_id, 'onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecac6dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2129,  2116,  2111,  2444,  1999,  4068,  1029,   102,  2129,\n",
       "          2116,  2111,  2444,  1999,  4068,  1029,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  4068,  2038,  1037,  2313,  1997,  1017,  1010, 19611,  1010,\n",
       "          6021,  2487,  5068,  4864,  1999,  2019,  2181,  1997,  6486,  2487,\n",
       "          1012,  6445,  2675,  7338,  1012,   102,  2047,  2259,  2103,  2003,\n",
       "          3297,  2005,  1996,  4956,  2688,  1997,  2396,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6eb5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from psutil import cpu_count\n",
    "from onnxruntime import InferenceSession, SessionOptions, get_all_providers\n",
    "\n",
    "environ[\"OMP_NUM_THREADS\"] = str(cpu_count(logical=True))\n",
    "environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
    "\n",
    "ort_session = InferenceSession(\"onnx/onnx/ms-marco-MiniLM-L-12-v2.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "ort_inputs = {k: v.cpu().detach().numpy() for k, v in features.items()}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20679ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[  101,  2129,  2116,  2111,  2444,  1999,  4068,  1029,   102,\n",
       "          2129,  2116,  2111,  2444,  1999,  4068,  1029,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  4068,  2038,  1037,  2313,  1997,  1017,  1010, 19611,\n",
       "          1010,  6021,  2487,  5068,  4864,  1999,  2019,  2181,  1997,\n",
       "          6486,  2487,  1012,  6445,  2675,  7338,  1012,   102,  2047,\n",
       "          2259,  2103,  2003,  3297,  2005,  1996,  4956,  2688,  1997,\n",
       "          2396,  1012,   102]]),\n",
       " 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43b90248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 39)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58470074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(2, 39, 384)\n"
     ]
    }
   ],
   "source": [
    "print(len(ort_outs))\n",
    "print(ort_outs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fc79f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002038736"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outs[0][0][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037aba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

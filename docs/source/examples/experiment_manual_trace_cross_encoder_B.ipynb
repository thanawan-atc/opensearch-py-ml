{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0347bac",
   "metadata": {},
   "source": [
    "# Trace Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "199e9c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████| 608/608 [00:00<00:00, 101kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 499M/499M [00:01<00:00, 296MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|█████| 142/142 [00:00<00:00, 82.6kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|███| 899k/899k [00:00<00:00, 12.0MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███| 456k/456k [00:00<00:00, 3.43MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████| 772/772 [00:00<00:00, 451kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_id = \"cross-encoder/stsb-roberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "features = tokenizer([('Query', 'Paragraph1'), ('Query', 'Paragraph2') , ('Query', 'Paragraph3')],  padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f46c3d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 48382,     2,     2, 22011, 44947,   134,     2],\n",
       "        [    0, 48382,     2,     2, 22011, 44947,   176,     2],\n",
       "        [    0, 48382,     2,     2, 22011, 44947,   246,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18d17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex_input = (features[\"input_ids\"], features[\"token_type_ids\"], features[\"attention_mask\"])\n",
    "ex_input = (features[\"input_ids\"], features[\"attention_mask\"])\n",
    "traced_model = torch.jit.trace(model, ex_input, strict=False)\n",
    "torch.jit.save(traced_model, \"traced_cross_encoder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3c6af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=RobertaForSequenceClassification\n",
       "  (roberta): RecursiveScriptModule(\n",
       "    original_name=RobertaModel\n",
       "    (embeddings): RecursiveScriptModule(\n",
       "      original_name=RobertaEmbeddings\n",
       "      (word_embeddings): RecursiveScriptModule(original_name=Embedding)\n",
       "      (position_embeddings): RecursiveScriptModule(original_name=Embedding)\n",
       "      (token_type_embeddings): RecursiveScriptModule(original_name=Embedding)\n",
       "      (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "      (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "    )\n",
       "    (encoder): RecursiveScriptModule(\n",
       "      original_name=RobertaEncoder\n",
       "      (layer): RecursiveScriptModule(\n",
       "        original_name=ModuleList\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=RobertaLayer\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=RobertaAttention\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfAttention\n",
       "              (query): RecursiveScriptModule(original_name=Linear)\n",
       "              (key): RecursiveScriptModule(original_name=Linear)\n",
       "              (value): RecursiveScriptModule(original_name=Linear)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfOutput\n",
       "              (dense): RecursiveScriptModule(original_name=Linear)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=RobertaIntermediate\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (intermediate_act_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=RobertaOutput\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=RobertaLayer\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=RobertaAttention\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfAttention\n",
       "              (query): RecursiveScriptModule(original_name=Linear)\n",
       "              (key): RecursiveScriptModule(original_name=Linear)\n",
       "              (value): RecursiveScriptModule(original_name=Linear)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfOutput\n",
       "              (dense): RecursiveScriptModule(original_name=Linear)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=RobertaIntermediate\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (intermediate_act_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=RobertaOutput\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (2): RecursiveScriptModule(\n",
       "          original_name=RobertaLayer\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=RobertaAttention\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfAttention\n",
       "              (query): RecursiveScriptModule(original_name=Linear)\n",
       "              (key): RecursiveScriptModule(original_name=Linear)\n",
       "              (value): RecursiveScriptModule(original_name=Linear)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfOutput\n",
       "              (dense): RecursiveScriptModule(original_name=Linear)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=RobertaIntermediate\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (intermediate_act_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=RobertaOutput\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (3): RecursiveScriptModule(\n",
       "          original_name=RobertaLayer\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=RobertaAttention\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfAttention\n",
       "              (query): RecursiveScriptModule(original_name=Linear)\n",
       "              (key): RecursiveScriptModule(original_name=Linear)\n",
       "              (value): RecursiveScriptModule(original_name=Linear)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfOutput\n",
       "              (dense): RecursiveScriptModule(original_name=Linear)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=RobertaIntermediate\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (intermediate_act_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=RobertaOutput\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (4): RecursiveScriptModule(\n",
       "          original_name=RobertaLayer\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=RobertaAttention\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfAttention\n",
       "              (query): RecursiveScriptModule(original_name=Linear)\n",
       "              (key): RecursiveScriptModule(original_name=Linear)\n",
       "              (value): RecursiveScriptModule(original_name=Linear)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfOutput\n",
       "              (dense): RecursiveScriptModule(original_name=Linear)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=RobertaIntermediate\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (intermediate_act_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=RobertaOutput\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (5): RecursiveScriptModule(\n",
       "          original_name=RobertaLayer\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=RobertaAttention\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfAttention\n",
       "              (query): RecursiveScriptModule(original_name=Linear)\n",
       "              (key): RecursiveScriptModule(original_name=Linear)\n",
       "              (value): RecursiveScriptModule(original_name=Linear)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfOutput\n",
       "              (dense): RecursiveScriptModule(original_name=Linear)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=RobertaIntermediate\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (intermediate_act_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=RobertaOutput\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (6): RecursiveScriptModule(\n",
       "          original_name=RobertaLayer\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=RobertaAttention\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfAttention\n",
       "              (query): RecursiveScriptModule(original_name=Linear)\n",
       "              (key): RecursiveScriptModule(original_name=Linear)\n",
       "              (value): RecursiveScriptModule(original_name=Linear)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfOutput\n",
       "              (dense): RecursiveScriptModule(original_name=Linear)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=RobertaIntermediate\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (intermediate_act_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=RobertaOutput\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (7): RecursiveScriptModule(\n",
       "          original_name=RobertaLayer\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=RobertaAttention\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfAttention\n",
       "              (query): RecursiveScriptModule(original_name=Linear)\n",
       "              (key): RecursiveScriptModule(original_name=Linear)\n",
       "              (value): RecursiveScriptModule(original_name=Linear)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfOutput\n",
       "              (dense): RecursiveScriptModule(original_name=Linear)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=RobertaIntermediate\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (intermediate_act_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=RobertaOutput\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (8): RecursiveScriptModule(\n",
       "          original_name=RobertaLayer\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=RobertaAttention\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfAttention\n",
       "              (query): RecursiveScriptModule(original_name=Linear)\n",
       "              (key): RecursiveScriptModule(original_name=Linear)\n",
       "              (value): RecursiveScriptModule(original_name=Linear)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfOutput\n",
       "              (dense): RecursiveScriptModule(original_name=Linear)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=RobertaIntermediate\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (intermediate_act_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=RobertaOutput\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (9): RecursiveScriptModule(\n",
       "          original_name=RobertaLayer\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=RobertaAttention\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfAttention\n",
       "              (query): RecursiveScriptModule(original_name=Linear)\n",
       "              (key): RecursiveScriptModule(original_name=Linear)\n",
       "              (value): RecursiveScriptModule(original_name=Linear)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfOutput\n",
       "              (dense): RecursiveScriptModule(original_name=Linear)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=RobertaIntermediate\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (intermediate_act_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=RobertaOutput\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (10): RecursiveScriptModule(\n",
       "          original_name=RobertaLayer\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=RobertaAttention\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfAttention\n",
       "              (query): RecursiveScriptModule(original_name=Linear)\n",
       "              (key): RecursiveScriptModule(original_name=Linear)\n",
       "              (value): RecursiveScriptModule(original_name=Linear)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfOutput\n",
       "              (dense): RecursiveScriptModule(original_name=Linear)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=RobertaIntermediate\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (intermediate_act_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=RobertaOutput\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "        (11): RecursiveScriptModule(\n",
       "          original_name=RobertaLayer\n",
       "          (attention): RecursiveScriptModule(\n",
       "            original_name=RobertaAttention\n",
       "            (self): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfAttention\n",
       "              (query): RecursiveScriptModule(original_name=Linear)\n",
       "              (key): RecursiveScriptModule(original_name=Linear)\n",
       "              (value): RecursiveScriptModule(original_name=Linear)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "            (output): RecursiveScriptModule(\n",
       "              original_name=RobertaSelfOutput\n",
       "              (dense): RecursiveScriptModule(original_name=Linear)\n",
       "              (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RecursiveScriptModule(\n",
       "            original_name=RobertaIntermediate\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (intermediate_act_fn): RecursiveScriptModule(original_name=GELUActivation)\n",
       "          )\n",
       "          (output): RecursiveScriptModule(\n",
       "            original_name=RobertaOutput\n",
       "            (dense): RecursiveScriptModule(original_name=Linear)\n",
       "            (LayerNorm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RecursiveScriptModule(\n",
       "    original_name=RobertaClassificationHead\n",
       "    (dense): RecursiveScriptModule(original_name=Linear)\n",
       "    (dropout): RecursiveScriptModule(original_name=Dropout)\n",
       "    (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = torch.jit.load(\"traced_cross_encoder.pt\")\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b541f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[ 5.6635],\n",
       "         [-3.7582]], grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [('How many people live in Berlin?', 'How many people live in Berlin?'), ('Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.')]\n",
    "test_features = tokenizer(test_sentences,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "pt_prediction = loaded_model(**test_features)\n",
    "pt_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb8037",
   "metadata": {},
   "source": [
    "# Compare Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b5924b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99654156, 0.02279299], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder(model_id)\n",
    "original_embedding = model.predict(test_sentences)\n",
    "original_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "045cb40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.6635],\n",
      "        [-3.7582]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "features = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d7f8642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(scores)\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:1216\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1216\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1228\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:845\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    853\u001b[0m     embedding_output,\n\u001b[1;32m    854\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    862\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    863\u001b[0m )\n\u001b[1;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:124\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings(input_ids)\n\u001b[0;32m--> 124\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_type_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, PreTrainedTokenizerFast\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"cross-encoder-torchscript/stsb-TinyBERT-L-4/tokenizer.json\")\n",
    "tokenizer.add_special_tokens({\n",
    "  \"cls_token\": \"[CLS]\",\n",
    "  \"mask_token\": \"[MASK]\",\n",
    "  \"pad_token\": \"[PAD]\",\n",
    "  \"sep_token\": \"[SEP]\",\n",
    "  \"unk_token\": \"[UNK]\"\n",
    "}\n",
    ")\n",
    "features = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3755e31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63385cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from mdutils.fileutils import MarkDownFile\n",
    "from zipfile import ZipFile\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.models import Normalize, Pooling, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55b82d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Run the three lines below\n",
    "model_id = \"hkunlp/instructor-large\"\n",
    "model = SentenceTransformer(model_id, cache_folder=\"cache_folder\")\n",
    "folder_path = \"sentence-transformer-torchscript\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2edef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file is saved to  sentence-transformer-torchscript/instructor-large.pt\n",
      "zip file is saved to  sentence-transformer-torchscript/instructor-large.zip \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sentence-transformer-torchscript/instructor-large.zip'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Go to cache_folder/hkunlp_instructor-large/1_Pooling/config.json \n",
    "# and remove \"pooling_mode_weightedmean_tokens\": false\" & \"pooling_mode_lasttoken\": false\n",
    "# as follows:\n",
    "# {\n",
    "#   \"word_embedding_dimension\": 768,\n",
    "#   \"pooling_mode_cls_token\": false,\n",
    "#   \"pooling_mode_mean_tokens\": true,\n",
    "#   \"pooling_mode_max_tokens\": false,\n",
    "#   \"pooling_mode_mean_sqrt_len_tokens\": false\n",
    "# }\n",
    "\n",
    "# 3. Run the function below so that you have model_zip_file saved at sentence-transformer-torchscript/instructor-large.zip\n",
    "def save_as_pt(\n",
    "    model,\n",
    "    model_id,\n",
    "    traced_folder,\n",
    "    sentences: [str],\n",
    ") -> str:\n",
    "    model_name = str(model_id.split(\"/\")[-1] + \".pt\")\n",
    "\n",
    "    model_path = os.path.join(traced_folder, model_name)\n",
    "    save_json_folder_path = traced_folder\n",
    "    model_output_path = traced_folder\n",
    "    zip_file_name = str(model_id.split(\"/\")[-1] + \".zip\")\n",
    "    zip_file_path = os.path.join(model_output_path, zip_file_name)\n",
    "\n",
    "    if model.tokenizer.model_max_length > model.get_max_seq_length():\n",
    "        model.tokenizer.model_max_length = model.get_max_seq_length()\n",
    "        print(\n",
    "            f\"The model_max_length is not properly defined in tokenizer_config.json. Setting it to be {model.tokenizer.model_max_length}\"\n",
    "        )\n",
    "\n",
    "    # save tokenizer.json in save_json_folder_name\n",
    "    model.save(save_json_folder_path)\n",
    "\n",
    "\n",
    "    # convert to pt format will need to be in cpu,\n",
    "    # set the device to cpu, convert its input_ids and attention_mask in cpu and save as .pt format\n",
    "    device = torch.device(\"cpu\")\n",
    "    cpu_model = model.to(device)\n",
    "    features = cpu_model.tokenizer(\n",
    "        sentences, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    compiled_model = torch.jit.trace(\n",
    "        cpu_model,\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": features[\"input_ids\"],\n",
    "                \"attention_mask\": features[\"attention_mask\"],\n",
    "            }\n",
    "        ),\n",
    "        strict=False,\n",
    "    )\n",
    "    torch.jit.save(compiled_model, model_path)\n",
    "    print(\"model file is saved to \", model_path)\n",
    "\n",
    "    # zip model file along with tokenizer.json as output\n",
    "    with ZipFile(str(zip_file_path), \"w\") as zipObj:\n",
    "        zipObj.write(\n",
    "            model_path,\n",
    "            arcname=str(model_name),\n",
    "        )\n",
    "        zipObj.write(\n",
    "            os.path.join(save_json_folder_path, \"tokenizer.json\"),\n",
    "            arcname=\"tokenizer.json\",\n",
    "        )\n",
    "    print(\"zip file is saved to \", zip_file_path, \"\\n\")\n",
    "    return zip_file_path\n",
    "\n",
    "save_as_pt(model=model, model_id=model_id, traced_folder=folder_path, sentences=[\"for example providing a small sentence\", \"we can add multiple sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11cb24c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-commons_model_config.json file is saved at :  sentence-transformer-torchscript/ml-commons_model_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sentence-transformer-torchscript/ml-commons_model_config.json'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Run the function below so that you have model_config_json_file saved at sentence-transformer-torchscript/ml-commons_model_config.json\n",
    "def _get_model_description_from_readme_file(model_id, readme_file_path) -> str:\n",
    "    readme_data = MarkDownFile.read_file(readme_file_path)\n",
    "\n",
    "    # Find the description section\n",
    "    start_str = f\"\\n# {model_id}\"\n",
    "    start = readme_data.find(start_str)\n",
    "    if start == -1:\n",
    "        model_name = model_id.split(\"/\")[1]\n",
    "        start_str = f\"\\n# {model_name}\"\n",
    "        start = readme_data.find(start_str)\n",
    "    end = readme_data.find(\"\\n#\", start + len(start_str))\n",
    "\n",
    "    # If we cannot find the scope of description section, raise error.\n",
    "    if start == -1 or end == -1:\n",
    "        assert False, \"Cannot find description in README.md file\"\n",
    "\n",
    "    # Parse out the description section\n",
    "    description = readme_data[start + len(start_str) + 1 : end].strip()\n",
    "    description = description.split(\"\\n\")[0]\n",
    "\n",
    "    # Remove hyperlink and reformat text\n",
    "    description = re.sub(r\"\\(.*?\\)\", \"\", description)\n",
    "    description = re.sub(r\"[\\[\\]]\", \"\", description)\n",
    "    description = re.sub(r\"\\*\", \"\", description)\n",
    "\n",
    "    # Remove unnecessary part if exists (i.e. \" For an introduction to ...\")\n",
    "    # (Found in https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1/blob/main/README.md)\n",
    "    unnecessary_part = description.find(\" For an introduction to\")\n",
    "    if unnecessary_part != -1:\n",
    "        description = description[:unnecessary_part]\n",
    "\n",
    "    return description\n",
    "\n",
    "def _generate_default_model_description(embedding_dimension) -> str:\n",
    "    \"\"\"\n",
    "    Generate default model description of the model based on embedding_dimension\n",
    "\n",
    "    ::param embedding_dimension: Embedding dimension of the model.\n",
    "    :type embedding_dimension: int\n",
    "    :return: Description of the model\n",
    "    :rtype: string\n",
    "    \"\"\"\n",
    "    print(\n",
    "            \"Using default description from embedding_dimension instead (You can overwrite this by specifying description parameter in make_model_config_json function\"\n",
    "    )\n",
    "    description = f\"This is a sentence-transformers model: It maps sentences & paragraphs to a {embedding_dimension} dimensional dense vector space.\"\n",
    "    return description\n",
    "    \n",
    "def make_model_config_json(\n",
    "    model,\n",
    "    model_id,\n",
    "    folder_path,\n",
    "    version_number: str = 1,\n",
    "    model_format: str = \"TORCH_SCRIPT\",\n",
    "    embedding_dimension: int = None,\n",
    "    pooling_mode: str = None,\n",
    "    normalize_result: bool = None,\n",
    "    description: str = None,\n",
    "    all_config: str = None,\n",
    "    model_type: str = None,\n",
    ") -> str:\n",
    "    config_json_file_path = os.path.join(folder_path, \"config.json\")\n",
    "    model_name = model_id\n",
    "\n",
    "    if (\n",
    "        model_type is None\n",
    "        or embedding_dimension is None\n",
    "        or pooling_mode is None\n",
    "        or normalize_result is None\n",
    "    ):\n",
    "        try:\n",
    "            if embedding_dimension is None:\n",
    "                embedding_dimension = model.get_sentence_embedding_dimension()\n",
    "\n",
    "            for str_idx, module in model._modules.items():\n",
    "                if model_type is None and isinstance(module, Transformer):\n",
    "                    model_type = module.auto_model.__class__.__name__\n",
    "                    model_type = model_type.lower().rstrip(\"model\")\n",
    "                elif pooling_mode is None and isinstance(module, Pooling):\n",
    "                    pooling_mode = module.get_pooling_mode_str().upper()\n",
    "                elif normalize_result is None and isinstance(module, Normalize):\n",
    "                        normalize_result = True\n",
    "            if normalize_result is None:\n",
    "                normalize_result = False\n",
    "        except Exception as e:\n",
    "            raise Exception(\n",
    "                f\"Raised exception while getting model data from pre-trained hugging-face model object: {e}\"\n",
    "            )\n",
    "\n",
    "    if description is None:\n",
    "        readme_file_path = os.path.join(folder_path, \"README.md\")\n",
    "        if os.path.exists(readme_file_path):\n",
    "            try:\n",
    "                description = _get_model_description_from_readme_file(\n",
    "                    model_id,\n",
    "                    readme_file_path\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Cannot scrape model description from README.md file: {e}\")\n",
    "                description = _generate_default_model_description(\n",
    "                    embedding_dimension\n",
    "                )\n",
    "        else:\n",
    "            print(\"Cannot find README.md file to scrape model description\")\n",
    "            description = _generate_default_model_description(\n",
    "                embedding_dimension\n",
    "            )\n",
    "\n",
    "    if all_config is None:\n",
    "        if not os.path.exists(config_json_file_path):\n",
    "            raise Exception(\n",
    "                str(\n",
    "                    \"Cannot find config.json in\"\n",
    "                    + config_json_file_path\n",
    "                    + \". Please check the config.son file in the path.\"\n",
    "                )\n",
    "            )\n",
    "        try:\n",
    "            with open(config_json_file_path) as f:\n",
    "                config_content = json.load(f)\n",
    "                if all_config is None:\n",
    "                    all_config = config_content\n",
    "        except IOError:\n",
    "            print(\n",
    "                \"Cannot open in config.json file at \",\n",
    "                config_json_file_path,\n",
    "                \". Please check the config.json \",\n",
    "                \"file in the path.\",\n",
    "            )\n",
    "\n",
    "    model_config_content = {\n",
    "        \"name\": model_name,\n",
    "        \"version\": version_number,\n",
    "        \"description\": description,\n",
    "        \"model_format\": model_format,\n",
    "        \"model_task_type\": \"TEXT_EMBEDDING\",\n",
    "        \"model_config\": {\n",
    "            \"model_type\": model_type,\n",
    "            \"embedding_dimension\": embedding_dimension,\n",
    "            \"framework_type\": \"sentence_transformers\",\n",
    "            \"pooling_mode\": pooling_mode,\n",
    "            \"normalize_result\": normalize_result,\n",
    "            \"all_config\": json.dumps(all_config),\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "    model_config_file_path = os.path.join(\n",
    "        folder_path, \"ml-commons_model_config.json\"\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(model_config_file_path), exist_ok=True)\n",
    "    with open(model_config_file_path, \"w\") as file:\n",
    "        json.dump(model_config_content, file)\n",
    "    print(\n",
    "        \"ml-commons_model_config.json file is saved at : \", model_config_file_path\n",
    "    )\n",
    "\n",
    "    return model_config_file_path\n",
    "\n",
    "make_model_config_json(model=model, model_id=model_id, folder_path=folder_path, model_format='TORCH_SCRIPT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7bd14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Use 'sentence-transformer-torchscript/instructor-large.zip' and 'sentence-transformer-torchscript/ml-commons_model_config.json' to register the model to OpenSearch cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf1062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b4b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45cc5394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file is saved to  sentence-transformer-torchscript/instructor-large.pt\n",
      "zip file is saved to  sentence-transformer-torchscript/instructor-large.zip \n",
      "\n",
      "ml-commons_model_config.json file is saved at :  sentence-transformer-torchscript/ml-commons_model_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sentence-transformer-torchscript/ml-commons_model_config.json'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import re\n",
    "# import torch\n",
    "# from mdutils.fileutils import MarkDownFile\n",
    "# from zipfile import ZipFile\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from sentence_transformers.models import Normalize, Pooling, Transformer\n",
    "\n",
    "\n",
    "# # 1. Run the three lines below\n",
    "# model_id = \"hkunlp/instructor-large\"\n",
    "# model = SentenceTransformer(model_id, cache_folder=\"cache_folder\")\n",
    "# folder_path = \"sentence-transformer-torchscript\"\n",
    "\n",
    "\n",
    "# # 2. Go to cache_folder/hkunlp_instructor-large/1_Pooling/config.json \n",
    "# # and remove \"pooling_mode_weightedmean_tokens\": false\" & \"pooling_mode_lasttoken\": false\n",
    "# # as follows:\n",
    "# # {\n",
    "# #   \"word_embedding_dimension\": 768,\n",
    "# #   \"pooling_mode_cls_token\": false,\n",
    "# #   \"pooling_mode_mean_tokens\": true,\n",
    "# #   \"pooling_mode_max_tokens\": false,\n",
    "# #   \"pooling_mode_mean_sqrt_len_tokens\": false\n",
    "# # }\n",
    "\n",
    "\n",
    "# # 3. Run the function below so that you have model_zip_file saved at sentence-transformer-torchscript/instructor-large.zip\n",
    "# def save_as_pt(\n",
    "#     model,\n",
    "#     model_id,\n",
    "#     traced_folder,\n",
    "#     sentences: [str],\n",
    "# ) -> str:\n",
    "#     model_name = str(model_id.split(\"/\")[-1] + \".pt\")\n",
    "\n",
    "#     model_path = os.path.join(traced_folder, model_name)\n",
    "#     save_json_folder_path = traced_folder\n",
    "#     model_output_path = traced_folder\n",
    "#     zip_file_name = str(model_id.split(\"/\")[-1] + \".zip\")\n",
    "#     zip_file_path = os.path.join(model_output_path, zip_file_name)\n",
    "\n",
    "#     if model.tokenizer.model_max_length > model.get_max_seq_length():\n",
    "#         model.tokenizer.model_max_length = model.get_max_seq_length()\n",
    "#         print(\n",
    "#             f\"The model_max_length is not properly defined in tokenizer_config.json. Setting it to be {model.tokenizer.model_max_length}\"\n",
    "#         )\n",
    "\n",
    "#     # save tokenizer.json in save_json_folder_name\n",
    "#     model.save(save_json_folder_path)\n",
    "\n",
    "\n",
    "#     # convert to pt format will need to be in cpu,\n",
    "#     # set the device to cpu, convert its input_ids and attention_mask in cpu and save as .pt format\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     cpu_model = model.to(device)\n",
    "#     features = cpu_model.tokenizer(\n",
    "#         sentences, return_tensors=\"pt\", padding=True, truncation=True\n",
    "#     ).to(device)\n",
    "\n",
    "#     compiled_model = torch.jit.trace(\n",
    "#         cpu_model,\n",
    "#         (\n",
    "#             {\n",
    "#                 \"input_ids\": features[\"input_ids\"],\n",
    "#                 \"attention_mask\": features[\"attention_mask\"],\n",
    "#             }\n",
    "#         ),\n",
    "#         strict=False,\n",
    "#     )\n",
    "#     torch.jit.save(compiled_model, model_path)\n",
    "#     print(\"model file is saved to \", model_path)\n",
    "\n",
    "#     # zip model file along with tokenizer.json as output\n",
    "#     with ZipFile(str(zip_file_path), \"w\") as zipObj:\n",
    "#         zipObj.write(\n",
    "#             model_path,\n",
    "#             arcname=str(model_name),\n",
    "#         )\n",
    "#         zipObj.write(\n",
    "#             os.path.join(save_json_folder_path, \"tokenizer.json\"),\n",
    "#             arcname=\"tokenizer.json\",\n",
    "#         )\n",
    "#     print(\"zip file is saved to \", zip_file_path, \"\\n\")\n",
    "#     return zip_file_path\n",
    "\n",
    "# save_as_pt(model=model, model_id=model_id, traced_folder=folder_path, sentences=[\"for example providing a small sentence\", \"we can add multiple sentences\"])\n",
    "\n",
    "\n",
    "# # 4. Run the function below so that you have model_config_json_file saved at sentence-transformer-torchscript/ml-commons_model_config.json\n",
    "# def _get_model_description_from_readme_file(model_id, readme_file_path) -> str:\n",
    "#     readme_data = MarkDownFile.read_file(readme_file_path)\n",
    "\n",
    "#     # Find the description section\n",
    "#     start_str = f\"\\n# {model_id}\"\n",
    "#     start = readme_data.find(start_str)\n",
    "#     if start == -1:\n",
    "#         model_name = model_id.split(\"/\")[1]\n",
    "#         start_str = f\"\\n# {model_name}\"\n",
    "#         start = readme_data.find(start_str)\n",
    "#     end = readme_data.find(\"\\n#\", start + len(start_str))\n",
    "\n",
    "#     # If we cannot find the scope of description section, raise error.\n",
    "#     if start == -1 or end == -1:\n",
    "#         assert False, \"Cannot find description in README.md file\"\n",
    "\n",
    "#     # Parse out the description section\n",
    "#     description = readme_data[start + len(start_str) + 1 : end].strip()\n",
    "#     description = description.split(\"\\n\")[0]\n",
    "\n",
    "#     # Remove hyperlink and reformat text\n",
    "#     description = re.sub(r\"\\(.*?\\)\", \"\", description)\n",
    "#     description = re.sub(r\"[\\[\\]]\", \"\", description)\n",
    "#     description = re.sub(r\"\\*\", \"\", description)\n",
    "\n",
    "#     # Remove unnecessary part if exists (i.e. \" For an introduction to ...\")\n",
    "#     # (Found in https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1/blob/main/README.md)\n",
    "#     unnecessary_part = description.find(\" For an introduction to\")\n",
    "#     if unnecessary_part != -1:\n",
    "#         description = description[:unnecessary_part]\n",
    "\n",
    "#     return description\n",
    "\n",
    "# def _generate_default_model_description(embedding_dimension) -> str:\n",
    "#     \"\"\"\n",
    "#     Generate default model description of the model based on embedding_dimension\n",
    "\n",
    "#     ::param embedding_dimension: Embedding dimension of the model.\n",
    "#     :type embedding_dimension: int\n",
    "#     :return: Description of the model\n",
    "#     :rtype: string\n",
    "#     \"\"\"\n",
    "#     print(\n",
    "#             \"Using default description from embedding_dimension instead (You can overwrite this by specifying description parameter in make_model_config_json function\"\n",
    "#     )\n",
    "#     description = f\"This is a sentence-transformers model: It maps sentences & paragraphs to a {embedding_dimension} dimensional dense vector space.\"\n",
    "#     return description\n",
    "    \n",
    "# def make_model_config_json(\n",
    "#     model,\n",
    "#     model_id,\n",
    "#     folder_path,\n",
    "#     version_number: str = 1,\n",
    "#     model_format: str = \"TORCH_SCRIPT\",\n",
    "#     embedding_dimension: int = None,\n",
    "#     pooling_mode: str = None,\n",
    "#     normalize_result: bool = None,\n",
    "#     description: str = None,\n",
    "#     all_config: str = None,\n",
    "#     model_type: str = None,\n",
    "# ) -> str:\n",
    "#     config_json_file_path = os.path.join(folder_path, \"config.json\")\n",
    "#     model_name = model_id\n",
    "\n",
    "#     if (\n",
    "#         model_type is None\n",
    "#         or embedding_dimension is None\n",
    "#         or pooling_mode is None\n",
    "#         or normalize_result is None\n",
    "#     ):\n",
    "#         try:\n",
    "#             if embedding_dimension is None:\n",
    "#                 embedding_dimension = model.get_sentence_embedding_dimension()\n",
    "\n",
    "#             for str_idx, module in model._modules.items():\n",
    "#                 if model_type is None and isinstance(module, Transformer):\n",
    "#                     model_type = module.auto_model.__class__.__name__\n",
    "#                     model_type = model_type.lower().rstrip(\"model\")\n",
    "#                 elif pooling_mode is None and isinstance(module, Pooling):\n",
    "#                     pooling_mode = module.get_pooling_mode_str().upper()\n",
    "#                 elif normalize_result is None and isinstance(module, Normalize):\n",
    "#                         normalize_result = True\n",
    "#             if normalize_result is None:\n",
    "#                 normalize_result = False\n",
    "#         except Exception as e:\n",
    "#             raise Exception(\n",
    "#                 f\"Raised exception while getting model data from pre-trained hugging-face model object: {e}\"\n",
    "#             )\n",
    "\n",
    "#     if description is None:\n",
    "#         readme_file_path = os.path.join(folder_path, \"README.md\")\n",
    "#         if os.path.exists(readme_file_path):\n",
    "#             try:\n",
    "#                 description = _get_model_description_from_readme_file(\n",
    "#                     model_id,\n",
    "#                     readme_file_path\n",
    "#                 )\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Cannot scrape model description from README.md file: {e}\")\n",
    "#                 description = _generate_default_model_description(\n",
    "#                     embedding_dimension\n",
    "#                 )\n",
    "#         else:\n",
    "#             print(\"Cannot find README.md file to scrape model description\")\n",
    "#             description = _generate_default_model_description(\n",
    "#                 embedding_dimension\n",
    "#             )\n",
    "\n",
    "#     if all_config is None:\n",
    "#         if not os.path.exists(config_json_file_path):\n",
    "#             raise Exception(\n",
    "#                 str(\n",
    "#                     \"Cannot find config.json in\"\n",
    "#                     + config_json_file_path\n",
    "#                     + \". Please check the config.son file in the path.\"\n",
    "#                 )\n",
    "#             )\n",
    "#         try:\n",
    "#             with open(config_json_file_path) as f:\n",
    "#                 config_content = json.load(f)\n",
    "#                 if all_config is None:\n",
    "#                     all_config = config_content\n",
    "#         except IOError:\n",
    "#             print(\n",
    "#                 \"Cannot open in config.json file at \",\n",
    "#                 config_json_file_path,\n",
    "#                 \". Please check the config.json \",\n",
    "#                 \"file in the path.\",\n",
    "#             )\n",
    "\n",
    "#     model_config_content = {\n",
    "#         \"name\": model_name,\n",
    "#         \"version\": version_number,\n",
    "#         \"description\": description,\n",
    "#         \"model_format\": model_format,\n",
    "#         \"model_task_type\": \"TEXT_EMBEDDING\",\n",
    "#         \"model_config\": {\n",
    "#             \"model_type\": model_type,\n",
    "#             \"embedding_dimension\": embedding_dimension,\n",
    "#             \"framework_type\": \"sentence_transformers\",\n",
    "#             \"pooling_mode\": pooling_mode,\n",
    "#             \"normalize_result\": normalize_result,\n",
    "#             \"all_config\": json.dumps(all_config),\n",
    "#         },\n",
    "#     }\n",
    "\n",
    "\n",
    "#     model_config_file_path = os.path.join(\n",
    "#         folder_path, \"ml-commons_model_config.json\"\n",
    "#     )\n",
    "#     os.makedirs(os.path.dirname(model_config_file_path), exist_ok=True)\n",
    "#     with open(model_config_file_path, \"w\") as file:\n",
    "#         json.dump(model_config_content, file)\n",
    "#     print(\n",
    "#         \"ml-commons_model_config.json file is saved at : \", model_config_file_path\n",
    "#     )\n",
    "\n",
    "#     return model_config_file_path\n",
    "\n",
    "# make_model_config_json(model=model, model_id=model_id, folder_path=folder_path, model_format='TORCH_SCRIPT')\n",
    "\n",
    "\n",
    "# # 5. Use 'sentence-transformer-torchscript/instructor-large.zip' and 'sentence-transformer-torchscript/ml-commons_model_config.json' to register the model to OpenSearch cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899152cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87f9b88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/opensearchpy/connection/http_urllib3.py:199: UserWarning: Connecting to https://localhost:9200 using SSL with verify_certs=False is insecure.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!pip install opensearch-py opensearch-py-ml\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../../..')))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"TracerWarning: torch.tensor\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"using SSL with verify_certs=False is insecure.\")\n",
    "\n",
    "import opensearch_py_ml as oml\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearch_py_ml.ml_models import SentenceTransformerModel\n",
    "# import mlcommon to later register the model to OpenSearch Cluster\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "\n",
    "CLUSTER_URL = 'https://localhost:9200'\n",
    "\n",
    "def get_os_client(cluster_url = CLUSTER_URL,\n",
    "                  username='admin',\n",
    "                  password='admin'):\n",
    "    '''\n",
    "    Get OpenSearch client\n",
    "    :param cluster_url: cluster URL like https://ml-te-netwo-1s12ba42br23v-ff1736fa7db98ff2.elb.us-west-2.amazonaws.com:443\n",
    "    :return: OpenSearch client\n",
    "    '''\n",
    "    client = OpenSearch(\n",
    "        hosts=[cluster_url],\n",
    "        http_auth=(username, password),\n",
    "        verify_certs=False\n",
    "    )\n",
    "    return client \n",
    "\n",
    "client = get_os_client()\n",
    "\n",
    "# Connect to ml_common client with OpenSearch client\n",
    "ml_client = MLCommonClient(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7e6d0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks 135\n",
      "Sha1 value of the model file:  41f576f7235aebd1b9ead05366b3fcb4e6c642ef225c9fbcb1d465356a24c53b\n",
      "Model meta data was created successfully. Model Id:  OJ-mbIoBx1PaKKd27EH_\n",
      "uploading chunk 1 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 2 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 3 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 4 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 5 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 6 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 7 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 8 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 9 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 10 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 11 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 12 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 13 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 14 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 15 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 16 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 17 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 18 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 19 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 20 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 21 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 22 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 23 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 24 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 25 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 26 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 27 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 28 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 29 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 30 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 31 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 32 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 33 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 34 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 35 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 36 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 37 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 38 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 39 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 40 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 41 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 42 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 43 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 44 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 45 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 46 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 47 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 48 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 49 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 50 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 51 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 52 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 53 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 54 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 55 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 56 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 57 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 58 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 59 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 60 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 61 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 62 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 63 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 64 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 65 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 66 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 67 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 68 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 69 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 70 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 71 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 72 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 73 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 74 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 75 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 76 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 77 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 78 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 79 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 80 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 81 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 82 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 83 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 84 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 85 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 86 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 87 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 88 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 89 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 90 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 91 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 92 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 93 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 94 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 95 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 96 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 97 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 98 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 99 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 100 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 101 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 102 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 103 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 104 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 105 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 106 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 107 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 108 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 109 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 110 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 111 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 112 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 113 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 114 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 115 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 116 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 117 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 118 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 119 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 120 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 121 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 122 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 123 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 124 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 125 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 126 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 127 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 128 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 129 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 130 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 131 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 132 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 133 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 134 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 135 of 135\n",
      "Model id: {'status': 'Uploaded'}\n",
      "Model registered successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: OZ-nbIoBx1PaKKd2qkFw\n",
      "Model deployed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OJ-mbIoBx1PaKKd27EH_'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.register_model(\"sentence-transformer-torchscript/instructor-large.zip\", \"sentence-transformer-torchscript/ml-commons_model_config.json\", isVerbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4444e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_sentences = [\"first sentence\", \"second sentence\"]\n",
    "\n",
    "# Generated embedding from torchScript\n",
    "\n",
    "embedding_output_torch = ml_client.generate_embedding(\"OJ-mbIoBx1PaKKd27EH_\", input_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85d0f4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "None\n",
      "1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "original_pre_trained_model = model\n",
    "original_embedding_data = list(\n",
    "    original_pre_trained_model.encode(input_sentences, convert_to_numpy=True)\n",
    ")\n",
    "        \n",
    "for i in range(len(input_sentences)):\n",
    "    print(i)\n",
    "    print(np.testing.assert_allclose(original_embedding_data[i], embedding_output_torch['inference_results'][i]['output'][0]['data'], rtol=1e-03, atol=1e-05))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a0804b",
   "metadata": {},
   "source": [
    "# Problems Regarding Tracing Model with Dense Layer in ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e9523",
   "metadata": {},
   "source": [
    "## Background Context\n",
    "\n",
    "We want to trace a pretrained sentence-transformer model into torch_script and onnx to upload it to the artifact server, so that the users can use the following ml-commons API to deploy the model and generate embeddings: `register` -> `deploy` -> `generate_embedding`. \n",
    "\n",
    "```\n",
    "import opensearch_py_ml as oml\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearch_py_ml.ml_models import SentenceTransformerModel\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "\n",
    "client = get_os_client()\n",
    "ml_client = MLCommonClient(client)\n",
    "\n",
    "model_id = \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "folder_path='sentence-transformers-onxx/distiluse-base-multilingual-cased-v1'\n",
    "\n",
    "pre_trained_model = SentenceTransformerModel(model_id=model_id, folder_path=folder_path, overwrite=True)\n",
    "model_path_onnx = pre_trained_model.save_as_onnx(model_id=model_id)\n",
    "\n",
    "model_config_path_onnx = 'sentence-transformers-onxx/distiluse-base-multilingual-cased-v1/ml-commons_model_config.json'\n",
    "ml_client.register_model(model_path_onnx, model_config_path_onnx, isVerbose=True)\n",
    "\n",
    "input_sentences = [\"first sentence\", \"second sentence\"]\n",
    "results = ml_client.generate_embedding(\"hv0c7IkBVsgBeq9g7M_J\", input_sentences)\n",
    "onnx_embedding = [\n",
    "            embedding_output_onnx[\"inference_results\"][i][\"output\"][0][\"data\"]\n",
    "            for i in range(len(input_sentences))\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "This should give the same output with the following Huggingface `encode` calls.\n",
    "```\n",
    "from sentence_transformers import SentenceTransformer\n",
    "input_sentences = [\"first sentence\", \"second sentence\"]\n",
    "\n",
    "huggingface_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v1')\n",
    "huggingface_embeddings = huggingface_model.encode(input_sentences)\n",
    "print(huggingface_embeddings)\n",
    "\n",
    "```\n",
    "\n",
    "Note that there is our `SentenceTransformerModel` class and Hugging Face `SentenceTransformer`class.\n",
    "\n",
    "The problem we face is that `onnx_embedding` has shape `(768,)`  while `huggingface_embeddings` has shape `(512,)`. However, we do not face this problem with `torch_script_embedding`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f42762",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "Based on the mismatch in output shape and the model architecture below, the problem is likely because the `Dense` layer is not part of the `onnx` model.\n",
    "\n",
    "```\n",
    "SentenceTransformer(\n",
    "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
    "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
    "  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
    ")\n",
    "```\n",
    "\n",
    "At first, I thought that the problem is with `convert` in `transformers.convert_graph_to_onnx` that we use to trace `onnx` file. However, I realize that for other models that have pooling layer as the last layer, their `onnx` file does not include pooling layer as well. Moreover, [this line of code](https://github.com/opensearch-project/ml-commons/blob/7467a692e3cf3d7cdf2b5db0b21c11e67fcf5621/ml-algorithms/src/main/java/org/opensearch/ml/engine/algorithms/text_embedding/ONNXSentenceTransformerTextEmbeddingTranslator.java#L83C19-L83C32) in ML-Commons shows that `onnx` model relies on post-processing in ML-commons to generate embedding from the model output (See example of how others load and generate embeddings by calling pooling function on model output [here](https://github.com/SidJain1412/sentence-transformers/blob/master/examples/onnx/onnx_example.ipynb)), while `torch_script` does not rely on this. Hence, the problem is we have not applied `dense` function. We should add it in post-processing step as we do for `pooling` and `normalize` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817e186",
   "metadata": {},
   "source": [
    "## Experiments with Post-Processing in Python\n",
    "\n",
    "Based on above findings, I tried loading `onnx` model and adding `pooling` and `dense` layers to its output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6805cb9a",
   "metadata": {},
   "source": [
    "### Case A: Models with Only Pooling — Working fine\n",
    "\n",
    "Below is the overview of the code for tracing models with only pooling layer in `onnx`. This works fine.\n",
    "\n",
    "I. Get Inputs\n",
    "```\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "input_sentences = [\"first sentence\", \"second sentence\", \"very very long random sentence for testing\"]\n",
    "autotokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "auto_features = autotokenizer(\n",
    "            input_sentences, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "```\n",
    "\n",
    "II. Load `onnx` model & Generate Ouputs\n",
    "```\n",
    "from os import environ\n",
    "from psutil import cpu_count\n",
    "from onnxruntime import InferenceSession, SessionOptions, get_all_providers\n",
    "\n",
    "environ[\"OMP_NUM_THREADS\"] = str(cpu_count(logical=True))\n",
    "environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
    "\n",
    "\n",
    "ort_session = InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "ort_inputs = {k: v.cpu().detach().numpy() for k, v in auto_features.items()}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "```\n",
    "\n",
    "III. Add Pooling Layer to Get Sentence Embeddings\n",
    "```\n",
    "import torch\n",
    "from sentence_transformers.models import Pooling\n",
    "\n",
    "pooling_layer = Pooling(768, pooling_mode_cls_token=True, pooling_mode_mean_tokens=False)\n",
    "features = {\n",
    "    'token_embeddings':  torch.from_numpy(ort_outs[0]),\n",
    "    'attention_mask': torch.from_numpy(ort_inputs['attention_mask'])\n",
    "}\n",
    "pooling_layer.forward(features)\n",
    "sentence_embeddings = features['sentence_embedding']\n",
    "\n",
    "embedding_data_onnx = [\n",
    "            sentence_embeddings[i]\n",
    "            for i in range(len(input_sentences))\n",
    "        ]\n",
    "```\n",
    "\n",
    "IV. Verify Embedding with Embeddings Encoded with Hugging Face Model \n",
    "```\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "original_pre_trained_model = SentenceTransformer(model_id) # From Huggingface\n",
    "original_embedding_data = list(\n",
    "    original_pre_trained_model.encode(input_sentences, convert_to_numpy=True)\n",
    ")\n",
    "        \n",
    "for i in range(len(input_sentences)):\n",
    "    print(i)\n",
    "    print(np.testing.assert_allclose(original_embedding_data[i], embedding_data_onnx[i], rtol=1e-03, atol=1e-05))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5ce2f",
   "metadata": {},
   "source": [
    "### Case B: Models with Pooling & Dense (First Attempt) — Failed\n",
    "\n",
    "```\n",
    "SentenceTransformer(\n",
    "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
    "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
    "  (2): Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n",
    ")\n",
    "```\n",
    "https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331c0416",
   "metadata": {},
   "source": [
    "#### 0. Trace the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f5999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('../../..')))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"TracerWarning: torch.tensor\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"using SSL with verify_certs=False is insecure.\")\n",
    "\n",
    "import opensearch_py_ml as oml\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearch_py_ml.ml_models import SentenceTransformerModel\n",
    "# import mlcommon to later register the model to OpenSearch Cluster\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3007e61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX opset version set to: 15\n",
      "Loading pipeline (model: sentence-transformers/distiluse-base-multilingual-cased-v1, tokenizer: sentence-transformers/distiluse-base-multilingual-cased-v1)\n",
      "Creating folder sentence-transformers-onxx/distiluse-base-multilingual-cased-v1/onnx\n",
      "Using framework PyTorch: 1.13.1+cu117\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch', 1: 'sequence'}\n",
      "Ensuring inputs are in correct order\n",
      "head_mask is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:223: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file is saved to  sentence-transformers-onxx/distiluse-base-multilingual-cased-v1/onnx/distiluse-base-multilingual-cased-v1.onnx\n",
      "zip file is saved to  sentence-transformers-onxx/distiluse-base-multilingual-cased-v1/distiluse-base-multilingual-cased-v1.zip \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers.convert_graph_to_onnx import convert\n",
    "\n",
    "model_id = \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    "folder_path='sentence-transformers-onxx/distiluse-base-multilingual-cased-v1'\n",
    "model_name = str(model_id.split(\"/\")[-1] + \".onnx\")\n",
    "model_path = os.path.join(folder_path, \"onnx\", model_name)\n",
    "\n",
    "# model = SentenceTransformer(model_id)\n",
    "# folder_path='sentence-transformers-onxx/distiluse-base-multilingual-cased-v1'\n",
    "\n",
    "# model_name = str(model_id.split(\"/\")[-1] + \".onnx\")\n",
    "\n",
    "# model_path = os.path.join(folder_path, \"onnx\", model_name)\n",
    "        \n",
    "# convert(\n",
    "#     framework=\"pt\",\n",
    "#     model=model_id,\n",
    "#     output=Path(model_path),\n",
    "#     opset=15,\n",
    "# )\n",
    "\n",
    "pre_trained_model = SentenceTransformerModel(folder_path=folder_path, overwrite=True)\n",
    "zip_path = pre_trained_model.save_as_onnx(model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c974bb",
   "metadata": {},
   "source": [
    "#### I. Get Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22afa595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 10422, 49219,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 11132, 49219,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 12558, 12558, 11695, 61952, 49219, 10142, 38306,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "input_sentences = [\"first sentence\", \"second sentence\", \"very very long random sentence for testing\"]\n",
    "autotokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "auto_features = autotokenizer(\n",
    "            input_sentences, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "auto_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b730a2",
   "metadata": {},
   "source": [
    "II. Load `onnx` model & Generate Ouputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48bcf005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from psutil import cpu_count\n",
    "from onnxruntime import InferenceSession, SessionOptions, get_all_providers\n",
    "\n",
    "environ[\"OMP_NUM_THREADS\"] = str(cpu_count(logical=True))\n",
    "environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
    "\n",
    "ort_session = InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "ort_inputs = {k: v.cpu().detach().numpy() for k, v in auto_features.items()}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ddafcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[  101, 10422, 49219,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 11132, 49219,   102,     0,     0,     0,     0,     0],\n",
       "        [  101, 12558, 12558, 11695, 61952, 49219, 10142, 38306,   102]]),\n",
       " 'attention_mask': array([[1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c356e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3, 9, 768)\n"
     ]
    }
   ],
   "source": [
    "print(len(ort_outs))\n",
    "print(ort_outs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbedc397",
   "metadata": {},
   "source": [
    "#### III. Add Pooling Layer (Mean Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9abc829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['token_embeddings', 'attention_mask', 'sentence_embedding'])\n",
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers.models import Pooling\n",
    "\n",
    "pooling_layer = Pooling(768, pooling_mode_mean_tokens=True)\n",
    "features = {\n",
    "    'token_embeddings':  torch.from_numpy(ort_outs[0]),\n",
    "    'attention_mask': torch.from_numpy(ort_inputs['attention_mask'])\n",
    "}\n",
    "pooling_layer.forward(features)\n",
    "print(features.keys())\n",
    "print(features['sentence_embedding'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9cee32",
   "metadata": {},
   "source": [
    "#### IV. Add Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "183c1471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_embeddings': tensor([[[-9.3187e-02,  4.3200e-02,  1.4325e-01,  ..., -1.1029e-01,\n",
       "            9.4209e-02, -3.6948e-03],\n",
       "          [-1.0118e-01,  2.4131e-02,  1.6269e-01,  ..., -7.6247e-02,\n",
       "            5.1720e-02,  2.4384e-02],\n",
       "          [-1.1288e-01,  1.0230e-01,  7.9914e-02,  ..., -9.0805e-02,\n",
       "            1.2735e-01, -6.1911e-02],\n",
       "          ...,\n",
       "          [-3.1557e-02, -1.0514e-03,  3.4721e-02,  ..., -1.7142e-01,\n",
       "            1.1406e-01,  2.1534e-02],\n",
       "          [-1.5114e-02,  3.9489e-02,  5.6255e-02,  ..., -1.1204e-01,\n",
       "            1.0426e-01, -1.5420e-04],\n",
       "          [-1.2633e-02,  1.5307e-02,  2.2421e-02,  ..., -1.2982e-01,\n",
       "            1.1010e-01,  9.1765e-03]],\n",
       " \n",
       "         [[-4.6544e-02,  5.0517e-02,  9.9694e-02,  ..., -4.3155e-02,\n",
       "            9.5299e-02,  3.0037e-02],\n",
       "          [-7.2471e-02,  8.0396e-02,  8.7466e-02,  ..., -2.0926e-02,\n",
       "            3.2088e-02,  3.8028e-02],\n",
       "          [-6.9155e-02,  1.0407e-01,  3.6066e-02,  ..., -2.7982e-02,\n",
       "            1.1376e-01, -1.7804e-02],\n",
       "          ...,\n",
       "          [-9.8446e-03,  8.0488e-03,  1.0714e-02,  ..., -1.2416e-01,\n",
       "            1.1877e-01,  5.3486e-02],\n",
       "          [ 2.6859e-03,  4.8247e-02,  3.1568e-02,  ..., -6.6897e-02,\n",
       "            1.0426e-01,  3.4586e-02],\n",
       "          [ 4.5734e-03,  2.7063e-02, -1.3287e-03,  ..., -8.6685e-02,\n",
       "            1.1291e-01,  4.4886e-02]],\n",
       " \n",
       "         [[-8.3932e-02, -7.5796e-03,  1.6676e-02,  ..., -5.9684e-03,\n",
       "            9.1174e-02,  9.0253e-02],\n",
       "          [ 3.0248e-02,  5.9548e-03,  2.1603e-02,  ..., -4.2555e-02,\n",
       "            5.6251e-02,  5.5946e-02],\n",
       "          [ 2.0581e-02,  1.7136e-02,  9.4890e-03,  ..., -4.9996e-02,\n",
       "            4.5288e-02,  6.5932e-02],\n",
       "          ...,\n",
       "          [-4.0498e-02, -2.3098e-02,  4.8018e-02,  ..., -5.3319e-03,\n",
       "            2.9976e-02,  1.3334e-01],\n",
       "          [ 1.4747e-02, -4.4017e-02,  4.9983e-02,  ...,  4.0377e-02,\n",
       "            9.9136e-02,  7.8462e-02],\n",
       "          [-1.6027e-02, -5.2859e-03,  4.3401e-02,  ..., -9.0955e-03,\n",
       "           -1.3513e-02,  3.1649e-02]]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'sentence_embedding': tensor([[ 0.0292,  0.0443, -0.0481,  ..., -0.0335,  0.0139,  0.0023],\n",
       "         [ 0.0472,  0.0150, -0.0057,  ..., -0.0218,  0.0338,  0.0156],\n",
       "         [ 0.0354,  0.0678, -0.0226,  ...,  0.0123,  0.0550, -0.0386]],\n",
       "        grad_fn=<TanhBackward0>)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers.models import Dense\n",
    "dense_layer = Dense(768, 512, bias=True, activation_function=torch.nn.modules.activation.Tanh())\n",
    "dense_layer.forward(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4113edd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['sentence_embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbe945c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_data_onnx = [\n",
    "            features['sentence_embedding'][i].cpu().detach().numpy()\n",
    "            for i in range(len(input_sentences))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646caa6",
   "metadata": {},
   "source": [
    "#### V. Verify Embedding with Embeddings Encoded with Hugging Face Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bb5363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=0.001, atol=1e-05\n\nMismatched elements: 512 / 512 (100%)\nMax absolute difference: 0.2255789\nMax relative difference: 53.520542\n x: array([ 1.097567e-02,  6.483248e-02, -4.571173e-02,  9.350104e-02,\n       -2.485733e-02, -3.051357e-02,  8.830560e-03,  1.258769e-02,\n        8.662871e-03, -4.904142e-02,  5.009779e-04, -6.247674e-03,...\n y: array([ 0.029242,  0.044274, -0.048109,  0.005339,  0.048379, -0.061276,\n        0.009061, -0.022101, -0.013753,  0.003824,  0.001493,  0.019158,\n        0.051583,  0.003758, -0.041029,  0.108596,  0.037499, -0.06195 ,...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_sentences)):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_embedding_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_data_onnx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-03\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-05\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/numpy/testing/_private/utils.py:862\u001b[0m, in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict)\u001b[0m\n\u001b[1;32m    858\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(remarks)\n\u001b[1;32m    859\u001b[0m         msg \u001b[38;5;241m=\u001b[39m build_err_msg([ox, oy], err_msg,\n\u001b[1;32m    860\u001b[0m                             verbose\u001b[38;5;241m=\u001b[39mverbose, header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m    861\u001b[0m                             names\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m), precision\u001b[38;5;241m=\u001b[39mprecision)\n\u001b[0;32m--> 862\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=0.001, atol=1e-05\n\nMismatched elements: 512 / 512 (100%)\nMax absolute difference: 0.2255789\nMax relative difference: 53.520542\n x: array([ 1.097567e-02,  6.483248e-02, -4.571173e-02,  9.350104e-02,\n       -2.485733e-02, -3.051357e-02,  8.830560e-03,  1.258769e-02,\n        8.662871e-03, -4.904142e-02,  5.009779e-04, -6.247674e-03,...\n y: array([ 0.029242,  0.044274, -0.048109,  0.005339,  0.048379, -0.061276,\n        0.009061, -0.022101, -0.013753,  0.003824,  0.001493,  0.019158,\n        0.051583,  0.003758, -0.041029,  0.108596,  0.037499, -0.06195 ,..."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "original_pre_trained_model = SentenceTransformer(model_id) # From Huggingface\n",
    "original_embedding_data = list(\n",
    "    original_pre_trained_model.encode(input_sentences, convert_to_numpy=True)\n",
    ")\n",
    "        \n",
    "for i in range(len(input_sentences)):\n",
    "    print(i)\n",
    "    print(np.testing.assert_allclose(original_embedding_data[i], embedding_data_onnx[i], rtol=1e-03, atol=1e-05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef7d98",
   "metadata": {},
   "source": [
    "### Case B: Models with Pooling & Dense (Second Attempt) — Succeeded\n",
    "\n",
    "Try changing how I initialized `Dense` layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0811d1",
   "metadata": {},
   "source": [
    "#### III. Add Pooling Layer (Mean Pooling) [No Change]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f31ff2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['token_embeddings', 'attention_mask', 'sentence_embedding'])\n",
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers.models import Pooling\n",
    "\n",
    "pooling_layer = Pooling(768, pooling_mode_mean_tokens=True)\n",
    "features_2 = {\n",
    "    'token_embeddings':  torch.from_numpy(ort_outs[0]),\n",
    "    'attention_mask': torch.from_numpy(ort_inputs['attention_mask'])\n",
    "}\n",
    "pooling_layer.forward(features_2)\n",
    "print(features_2.keys())\n",
    "print(features_2['sentence_embedding'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a1037",
   "metadata": {},
   "source": [
    "#### IV. Add Dense Layer with `load()` method of `Dense` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "328bb1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_embeddings': tensor([[[-9.3187e-02,  4.3200e-02,  1.4325e-01,  ..., -1.1029e-01,\n",
       "            9.4209e-02, -3.6948e-03],\n",
       "          [-1.0118e-01,  2.4131e-02,  1.6269e-01,  ..., -7.6247e-02,\n",
       "            5.1720e-02,  2.4384e-02],\n",
       "          [-1.1288e-01,  1.0230e-01,  7.9914e-02,  ..., -9.0805e-02,\n",
       "            1.2735e-01, -6.1911e-02],\n",
       "          ...,\n",
       "          [-3.1557e-02, -1.0514e-03,  3.4721e-02,  ..., -1.7142e-01,\n",
       "            1.1406e-01,  2.1534e-02],\n",
       "          [-1.5114e-02,  3.9489e-02,  5.6255e-02,  ..., -1.1204e-01,\n",
       "            1.0426e-01, -1.5420e-04],\n",
       "          [-1.2633e-02,  1.5307e-02,  2.2421e-02,  ..., -1.2982e-01,\n",
       "            1.1010e-01,  9.1765e-03]],\n",
       " \n",
       "         [[-4.6544e-02,  5.0517e-02,  9.9694e-02,  ..., -4.3155e-02,\n",
       "            9.5299e-02,  3.0037e-02],\n",
       "          [-7.2471e-02,  8.0396e-02,  8.7466e-02,  ..., -2.0926e-02,\n",
       "            3.2088e-02,  3.8028e-02],\n",
       "          [-6.9155e-02,  1.0407e-01,  3.6066e-02,  ..., -2.7982e-02,\n",
       "            1.1376e-01, -1.7804e-02],\n",
       "          ...,\n",
       "          [-9.8446e-03,  8.0488e-03,  1.0714e-02,  ..., -1.2416e-01,\n",
       "            1.1877e-01,  5.3486e-02],\n",
       "          [ 2.6859e-03,  4.8247e-02,  3.1568e-02,  ..., -6.6897e-02,\n",
       "            1.0426e-01,  3.4586e-02],\n",
       "          [ 4.5734e-03,  2.7063e-02, -1.3287e-03,  ..., -8.6685e-02,\n",
       "            1.1291e-01,  4.4886e-02]],\n",
       " \n",
       "         [[-8.3932e-02, -7.5796e-03,  1.6676e-02,  ..., -5.9684e-03,\n",
       "            9.1174e-02,  9.0253e-02],\n",
       "          [ 3.0248e-02,  5.9548e-03,  2.1603e-02,  ..., -4.2555e-02,\n",
       "            5.6251e-02,  5.5946e-02],\n",
       "          [ 2.0581e-02,  1.7136e-02,  9.4890e-03,  ..., -4.9996e-02,\n",
       "            4.5288e-02,  6.5932e-02],\n",
       "          ...,\n",
       "          [-4.0498e-02, -2.3098e-02,  4.8018e-02,  ..., -5.3319e-03,\n",
       "            2.9976e-02,  1.3334e-01],\n",
       "          [ 1.4747e-02, -4.4017e-02,  4.9983e-02,  ...,  4.0377e-02,\n",
       "            9.9136e-02,  7.8462e-02],\n",
       "          [-1.6027e-02, -5.2859e-03,  4.3401e-02,  ..., -9.0955e-03,\n",
       "           -1.3513e-02,  3.1649e-02]]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'sentence_embedding': tensor([[ 0.0110,  0.0648, -0.0457,  ..., -0.0498,  0.0185, -0.0380],\n",
       "         [-0.0012,  0.0392, -0.0332,  ..., -0.0417,  0.0375, -0.0652],\n",
       "         [ 0.0700,  0.0559, -0.0231,  ..., -0.0752, -0.0121,  0.0374]],\n",
       "        grad_fn=<TanhBackward0>)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers.models import Dense\n",
    "loaded_dense_layer = Dense.load(folder_path + '/2_Dense')\n",
    "loaded_dense_layer.forward(features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29178ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_2['sentence_embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ec0867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_data_onnx_2 = [\n",
    "            features_2['sentence_embedding'][i].cpu().detach().numpy()\n",
    "            for i in range(len(input_sentences))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f5586",
   "metadata": {},
   "source": [
    "#### V. Verify Embedding with Embeddings Encoded with Hugging Face Model  [No Change]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "463155d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "None\n",
      "1\n",
      "None\n",
      "2\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "original_pre_trained_model = SentenceTransformer(model_id) # From Huggingface\n",
    "original_embedding_data = list(\n",
    "    original_pre_trained_model.encode(input_sentences, convert_to_numpy=True)\n",
    ")\n",
    "        \n",
    "for i in range(len(input_sentences)):\n",
    "    print(i)\n",
    "    print(np.testing.assert_allclose(original_embedding_data[i], embedding_data_onnx_2[i], rtol=1e-03, atol=1e-05))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a99494",
   "metadata": {},
   "source": [
    "## Questions \n",
    "* Why do we need to use `load()` method? Why isn't initializing `Dense()` with parameters sufficient?\n",
    "* What is in [`2_Dense/pytorch_model.bin`](https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1/tree/main/2_Dense)?\n",
    "* Does it mean that we should upload `2_Dense/pytorch_model.bin` to model hub as well apart from `.onnx` and `tokenizer.json`?\n",
    "\n",
    "```\n",
    "dense_layer = Dense(768, 512, bias=True, activation_function=torch.nn.modules.activation.Tanh())\n",
    "loaded_dense_layer = Dense.load(folder_path + '/2_Dense')\n",
    "```\n",
    "Note:\n",
    "- See `Dense` class definition here: https://github.com/SidJain1412/sentence-transformers/blob/master/sentence_transformers/models/Dense.py)\n",
    "- The file at `folder_path + '/2_Dense'` is https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1/tree/main/2_Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c62a82d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ce66653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense({'in_features': 768, 'out_features': 512, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dense_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f6d26",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "\n",
    "https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/models/Dense.py#L32\n",
    "https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb\n",
    "https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
